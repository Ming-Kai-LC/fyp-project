{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - CrossViT Training (Phase 2)\n",
    "\n",
    "**Author:** Tan Ming Kai (24PMR12003)  \n",
    "**Date:** 2025-11-12  \n",
    "**Purpose:** Train CrossViT-Tiny with 5 random seeds for statistical validation\n",
    "\n",
    "**Project:** Multi-Scale Vision Transformer (CrossViT) for COVID-19 Chest X-ray Classification  \n",
    "**Academic Year:** 2025/26\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "1. ‚úÖ Train CrossViT-Tiny with 5 different random seeds (42, 123, 456, 789, 101112)\n",
    "2. ‚úÖ Log all runs to MLflow for experiment tracking\n",
    "3. ‚úÖ Save model checkpoints and confusion matrices\n",
    "4. ‚úÖ Calculate mean ¬± std accuracy across seeds\n",
    "5. ‚úÖ Generate results table for thesis Chapter 5\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 2: Systematic Experimentation\n",
    "\n",
    "This notebook is part of Phase 2, where we train ALL 6 models with 5 seeds each (30 total runs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reproducibility Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CrossViT Training Notebook for COVID-19 FYP\n",
    "Author: Tan Ming Kai (24PMR12003)\n",
    "Purpose: Train CrossViT-Tiny with multiple random seeds for statistical validation\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 1. STANDARD LIBRARY IMPORTS\n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DATA SCIENCE LIBRARIES\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure display\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# ============================================================================\n",
    "# 3. PYTORCH & DEEP LEARNING\n",
    "# ============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# ============================================================================\n",
    "# 4. TIMM (PyTorch Image Models) for CrossViT\n",
    "# ============================================================================\n",
    "import timm\n",
    "\n",
    "# ============================================================================\n",
    "# 5. COMPUTER VISION\n",
    "# ============================================================================\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# ============================================================================\n",
    "# 6. MLFLOW (Experiment Tracking)\n",
    "# ============================================================================\n",
    "try:\n",
    "    import mlflow\n",
    "    import mlflow.pytorch\n",
    "    MLFLOW_AVAILABLE = True\n",
    "    print(\"‚úÖ MLflow available for experiment tracking\")\n",
    "except ImportError:\n",
    "    MLFLOW_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  MLflow not installed. Install with: pip install mlflow\")\n",
    "    print(\"   Continuing without experiment tracking...\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. SKLEARN (Metrics)\n",
    "# ============================================================================\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Timm version: {timm.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hardware Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"HARDWARE VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check CUDA\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if cuda_available else 'cpu')\n",
    "\n",
    "print(f\"\\n‚úì CUDA Available: {cuda_available}\")\n",
    "print(f\"‚úì Using Device: {device}\")\n",
    "\n",
    "if cuda_available:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"‚úì GPU: {gpu_name}\")\n",
    "    print(f\"‚úì Total VRAM: {total_memory:.2f} GB\")\n",
    "    print(f\"‚úì CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Memory monitoring function\n",
    "    def print_gpu_memory(prefix=\"\"):\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        free = total_memory - reserved\n",
    "        print(f\"{prefix}GPU Memory: Allocated={allocated:.3f}GB | Reserved={reserved:.3f}GB | Free={free:.3f}GB\")\n",
    "    \n",
    "    print_gpu_memory(\"\\n  \")\n",
    "    \n",
    "    if \"4060\" in gpu_name and 7.0 <= total_memory <= 9.0:\n",
    "        print(\"\\n‚úÖ CONFIRMED: RTX 4060 8GB detected - Ready for CrossViT training!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Different GPU detected: {gpu_name}\")\n",
    "        print(\"   Adjust batch size if needed based on VRAM.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå WARNING: No GPU detected! Training will be VERY slow.\")\n",
    "    print(\"   Please ensure CUDA drivers and PyTorch with CUDA are installed.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "**CRITICAL:** These hyperparameters are FIXED per CLAUDE.md specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "CSV_DIR = Path(\"../data/processed\")\n",
    "PROCESSED_IMG_DIR = Path(\"../data/processed/clahe_enhanced\")\n",
    "MODELS_DIR = Path(\"../models\")\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "\n",
    "# Create directories\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training configuration (FIXED per CLAUDE.md)\n",
    "BASE_CONFIG = {\n",
    "    # Device\n",
    "    'device': device,\n",
    "    \n",
    "    # Model\n",
    "    'model_name': 'CrossViT-Tiny',\n",
    "    'timm_model': 'crossvit_tiny_240',\n",
    "    'num_classes': 4,\n",
    "    'pretrained': True,\n",
    "    \n",
    "    # Data\n",
    "    'image_size': 240,\n",
    "    'class_names': ['COVID', 'Normal', 'Lung_Opacity', 'Viral Pneumonia'],\n",
    "    'class_weights': [1.47, 0.52, 0.88, 3.95],  # From EDA\n",
    "    \n",
    "    # Training hyperparameters - FIXED per CLAUDE.md\n",
    "    'batch_size': 8,  # Reduced for CrossViT (safer on 8GB VRAM)\n",
    "    'gradient_accumulation_steps': 4,  # Effective batch size = 32\n",
    "    'num_workers': 0,  # Must be 0 on Windows\n",
    "    'pin_memory': False,  # Disabled on Windows\n",
    "    'persistent_workers': False,\n",
    "    \n",
    "    # Optimizer - FIXED per CLAUDE.md\n",
    "    'learning_rate': 5e-5,  # CrossViT-specific\n",
    "    'weight_decay': 0.05,  # CrossViT-specific\n",
    "    'max_epochs': 50,\n",
    "    'early_stopping_patience': 15,\n",
    "    \n",
    "    # ImageNet normalization\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225],\n",
    "    \n",
    "    # Memory management\n",
    "    'mixed_precision': True,\n",
    "    \n",
    "    # Multi-seed experiment\n",
    "    'seeds': [42, 123, 456, 789, 101112],  # 5 seeds for statistical validation\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CROSSVIT TRAINING CONFIGURATION (FIXED HYPERPARAMETERS)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n‚úì Model: {BASE_CONFIG['model_name']}\")\n",
    "print(f\"‚úì Timm Model: {BASE_CONFIG['timm_model']}\")\n",
    "print(f\"‚úì Device: {BASE_CONFIG['device']}\")\n",
    "print(f\"‚úì Batch Size: {BASE_CONFIG['batch_size']} (gradient accumulation: {BASE_CONFIG['gradient_accumulation_steps']})\")\n",
    "print(f\"‚úì Effective Batch Size: {BASE_CONFIG['batch_size'] * BASE_CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"‚úì Learning Rate: {BASE_CONFIG['learning_rate']}\")\n",
    "print(f\"‚úì Weight Decay: {BASE_CONFIG['weight_decay']}\")\n",
    "print(f\"‚úì Max Epochs: {BASE_CONFIG['max_epochs']}\")\n",
    "print(f\"‚úì Early Stopping Patience: {BASE_CONFIG['early_stopping_patience']}\")\n",
    "print(f\"‚úì Image Size: {BASE_CONFIG['image_size']}√ó{BASE_CONFIG['image_size']}\")\n",
    "print(f\"‚úì Mixed Precision: {BASE_CONFIG['mixed_precision']}\")\n",
    "print(f\"\\n‚úì Random Seeds: {BASE_CONFIG['seeds']}\")\n",
    "print(f\"  ‚Üí Will train {len(BASE_CONFIG['seeds'])} times for statistical validation\")\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT: These hyperparameters are FIXED per CLAUDE.md\")\n",
    "print(\"   Do not modify unless explicitly required.\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MLflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MLFLOW EXPERIMENT TRACKING SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if MLFLOW_AVAILABLE:\n",
    "    # Set experiment name\n",
    "    mlflow.set_experiment(\"crossvit-covid19-classification\")\n",
    "    \n",
    "    # Set tracking URI (local directory)\n",
    "    mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "    \n",
    "    print(\"\\n‚úÖ MLflow configured:\")\n",
    "    print(f\"   - Experiment: crossvit-covid19-classification\")\n",
    "    print(f\"   - Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "    print(f\"\\nüí° View results: Run 'mlflow ui' in terminal, then open http://localhost:5000\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  MLflow not available. Results will not be logged.\")\n",
    "    print(\"   Install with: pip install mlflow\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING DATA SPLITS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load processed CSV files\n",
    "train_df = pd.read_csv(CSV_DIR / \"train_processed.csv\")\n",
    "val_df = pd.read_csv(CSV_DIR / \"val_processed.csv\")\n",
    "test_df = pd.read_csv(CSV_DIR / \"test_processed.csv\")\n",
    "\n",
    "print(f\"\\n‚úÖ CSV files loaded:\")\n",
    "print(f\"   - Train: {len(train_df):,} images\")\n",
    "print(f\"   - Val:   {len(val_df):,} images\")\n",
    "print(f\"   - Test:  {len(test_df):,} images\")\n",
    "\n",
    "print(\"\\nüìä Class Distribution in Training Set:\")\n",
    "class_counts = train_df['class_name'].value_counts()\n",
    "for class_name, count in class_counts.items():\n",
    "    pct = count / len(train_df) * 100\n",
    "    print(f\"   {class_name:20s}: {count:5d} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COVID19Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for COVID-19 chest X-ray classification.\n",
    "    \n",
    "    Loads CLAHE-enhanced images (240√ó240√ó3 RGB) from preprocessed directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame with 'processed_path' and 'label' columns\n",
    "            transform (callable, optional): Transformations to apply to images\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Extract paths and labels\n",
    "        self.image_paths = self.dataframe['processed_path'].values\n",
    "        self.labels = self.dataframe['label'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and return image and label at index idx.\n",
    "        \"\"\"\n",
    "        # Load image (BGR format from cv2)\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert to PIL Image for torchvision transforms\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get label\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "print(\"‚úÖ COVID19Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define Data Transforms\n",
    "\n",
    "Using **conservative augmentation** as per CLAUDE.md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms (conservative augmentation per CLAUDE.md)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((BASE_CONFIG['image_size'], BASE_CONFIG['image_size'])),\n",
    "    transforms.RandomRotation(10),  # ¬±10¬∞ only\n",
    "    transforms.RandomHorizontalFlip(0.5),  # NO vertical flip\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=BASE_CONFIG['mean'], std=BASE_CONFIG['std'])\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((BASE_CONFIG['image_size'], BASE_CONFIG['image_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=BASE_CONFIG['mean'], std=BASE_CONFIG['std'])\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Data transforms defined (Conservative augmentation per CLAUDE.md)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Set all random seeds for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, scaler=None, \n",
    "                   gradient_accumulation_steps=1, epoch=0):\n",
    "    \"\"\"\n",
    "    Train for one epoch with gradient accumulation.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1} [Train]\")\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(progress_bar):\n",
    "        # Move to device\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss = loss / gradient_accumulation_steps  # Scale loss\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Update weights every gradient_accumulation_steps\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * gradient_accumulation_steps\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': running_loss / (batch_idx + 1),\n",
    "            'acc': 100. * correct / total\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device, desc=\"Val\"):\n",
    "    \"\"\"\n",
    "    Validate model on validation/test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(loader, desc=f\"[{desc}]\")\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Store for metrics\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': running_loss / (progress_bar.n + 1),\n",
    "                'acc': 100. * correct / total\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training and validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Single Seed Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crossvit_single_seed(seed, base_config):\n",
    "    \"\"\"\n",
    "    Train CrossViT with a single random seed.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results containing test_acc, test_loss, confusion_matrix, etc.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"TRAINING CROSSVIT WITH SEED {seed}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Set seed\n",
    "    set_seed(seed)\n",
    "    print(f\"\\n‚úÖ Random seed set to {seed}\")\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = COVID19Dataset(train_df, transform=train_transform)\n",
    "    val_dataset = COVID19Dataset(val_df, transform=val_transform)\n",
    "    test_dataset = COVID19Dataset(test_df, transform=val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = 88\n",
    "        shuffle=True,\n",
    "        num_workers=base_config['num_workers'],\n",
    "        pin_memory=base_config['pin_memory'],\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size = 88\n",
    "        shuffle=False,\n",
    "        num_workers=base_config['num_workers'],\n",
    "        pin_memory=base_config['pin_memory']\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size = 88\n",
    "        shuffle=False,\n",
    "        num_workers=base_config['num_workers'],\n",
    "        pin_memory=base_config['pin_memory']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ DataLoaders created: {len(train_loader)} train batches\")\n",
    "    \n",
    "    # Load CrossViT model\n",
    "    model = timm.create_model(\n",
    "        base_config['timm_model'],\n",
    "        pretrained=base_config['pretrained'],\n",
    "        num_classes=base_config['num_classes']\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"‚úÖ CrossViT loaded: {total_params:,} parameters\")\n",
    "    \n",
    "    # Loss, optimizer, scheduler\n",
    "    class_weights = torch.tensor(base_config['class_weights'], dtype=torch.float32).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=base_config['learning_rate'],\n",
    "        weight_decay=base_config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=10,\n",
    "        T_mult=2\n",
    "    )\n",
    "    \n",
    "    # Mixed precision scaler\n",
    "    scaler = torch.cuda.amp.GradScaler() if base_config['mixed_precision'] else None\n",
    "    \n",
    "    # Start MLflow run\n",
    "    if MLFLOW_AVAILABLE:\n",
    "        run_name = f\"crossvit-seed-{seed}\"\n",
    "        mlflow.start_run(run_name=run_name)\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"model\", base_config['model_name'])\n",
    "        mlflow.log_param(\"random_seed\", seed)\n",
    "        mlflow.log_param(\"batch_size\", base_config['batch_size'])\n",
    "        mlflow.log_param(\"gradient_accumulation_steps\", base_config['gradient_accumulation_steps'])\n",
    "        mlflow.log_param(\"effective_batch_size\", base_config['batch_size'] * base_config['gradient_accumulation_steps'])\n",
    "        mlflow.log_param(\"learning_rate\", base_config['learning_rate'])\n",
    "        mlflow.log_param(\"weight_decay\", base_config['weight_decay'])\n",
    "        mlflow.log_param(\"max_epochs\", base_config['max_epochs'])\n",
    "        mlflow.set_tag(\"phase\", \"Phase 2 - Systematic Experimentation\")\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_path = MODELS_DIR / f\"crossvit_best_seed{seed}.pth\"\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(base_config['max_epochs']):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, scaler,\n",
    "            base_config['gradient_accumulation_steps'], epoch\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Log to MLflow\n",
    "        if MLFLOW_AVAILABLE:\n",
    "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"train_acc\", train_acc, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_acc\", val_acc, step=epoch)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}: Train Loss={train_loss:.4f} | Val Loss={val_loss:.4f} | Val Acc={val_acc:.2f}%\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"‚úÖ Best model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= base_config['early_stopping_patience']:\n",
    "                print(f\"\\n‚èπÔ∏è  Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    test_loss, test_acc, test_preds, test_labels = validate(\n",
    "        model, test_loader, criterion, device, desc=\"Test\"\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(test_labels, test_preds)\n",
    "    \n",
    "    # Save confusion matrix plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt='d', cmap='Blues',\n",
    "        xticklabels=base_config['class_names'],\n",
    "        yticklabels=base_config['class_names']\n",
    "    )\n",
    "    plt.ylabel('True Label', fontweight='bold')\n",
    "    plt.xlabel('Predicted Label', fontweight='bold')\n",
    "    plt.title(f\"CrossViT Confusion Matrix (Seed {seed})\", fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    cm_path = RESULTS_DIR / f\"crossvit_cm_seed{seed}.png\"\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Log final results\n",
    "    if MLFLOW_AVAILABLE:\n",
    "        mlflow.log_metric(\"test_loss\", test_loss)\n",
    "        mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "        mlflow.log_metric(\"training_time_minutes\", training_time / 60)\n",
    "        mlflow.log_artifact(str(cm_path))\n",
    "        mlflow.end_run()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Seed {seed} complete: Test Acc = {test_acc:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'seed': seed,\n",
    "        'test_acc': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'confusion_matrix': cm,\n",
    "        'training_time': training_time,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Single seed training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train CrossViT with All Seeds\n",
    "\n",
    "**This will train 5 times (seeds: 42, 123, 456, 789, 101112)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"  * 70)\n",
    "print(\"STARTING MULTI-SEED CROSSVIT TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìä Will train CrossViT {len(BASE_CONFIG['seeds'])} times with different seeds\")\n",
    "print(f\"   Seeds: {BASE_CONFIG['seeds']}\")\n",
    "print(f\"\\n‚è±Ô∏è  Estimated time: ~2-3 hours per seed (~10-15 hours total)\")\n",
    "print(f\"\\nüöÄ Starting training...\\n\")\n",
    "\n",
    "# Train with all seeds\n",
    "all_results = []\n",
    "\n",
    "for seed in BASE_CONFIG['seeds']:\n",
    "    try:\n",
    "        result = train_crossvit_single_seed(seed, BASE_CONFIG)\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR training with seed {seed}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL SEEDS TRAINING COMPLETED\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Statistical Analysis\n",
    "\n",
    "Calculate mean ¬± std accuracy across all seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract accuracies\n",
    "accuracies = [r['test_acc'] for r in all_results]\n",
    "seeds = [r['seed'] for r in all_results]\n",
    "\n",
    "# Calculate statistics\n",
    "mean_acc = np.mean(accuracies)\n",
    "std_acc = np.std(accuracies, ddof=1)  # Sample std\n",
    "min_acc = np.min(accuracies)\n",
    "max_acc = np.max(accuracies)\n",
    "\n",
    "print(f\"\\nüìä CrossViT Test Accuracy (5 seeds):\")\n",
    "print(f\"   Mean ¬± Std: {mean_acc:.2f}% ¬± {std_acc:.2f}%\")\n",
    "print(f\"   Range: [{min_acc:.2f}%, {max_acc:.2f}%]\")\n",
    "print(f\"\\nüìã Individual Results:\")\n",
    "for seed, acc in zip(seeds, accuracies):\n",
    "    print(f\"   Seed {seed:6d}: {acc:.2f}%\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['CrossViT-Tiny'] * len(all_results),\n",
    "    'Seed': seeds,\n",
    "    'Test Accuracy (%)': accuracies,\n",
    "    'Test Loss': [r['test_loss'] for r in all_results],\n",
    "    'Training Time (min)': [r['training_time'] / 60 for r in all_results]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "results_path = RESULTS_DIR / \"crossvit_results.csv\"\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"\\n‚úÖ Results saved to: {results_path}\")\n",
    "\n",
    "# Display table\n",
    "print(f\"\\n{results_df.to_string(index=False)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CROSSVIT TRAINING - SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚úÖ COMPLETED:\")\n",
    "print(f\"   [‚úì] Trained CrossViT-Tiny with {len(BASE_CONFIG['seeds'])} random seeds\")\n",
    "print(f\"   [‚úì] Logged all runs to MLflow\")\n",
    "print(f\"   [‚úì] Saved {len(all_results)} model checkpoints\")\n",
    "print(f\"   [‚úì] Generated {len(all_results)} confusion matrices\")\n",
    "print(f\"   [‚úì] Calculated statistics (mean ¬± std)\")\n",
    "\n",
    "print(f\"\\nüìä FINAL STATISTICS:\")\n",
    "print(f\"   CrossViT-Tiny: {mean_acc:.2f}% ¬± {std_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUT FILES:\")\n",
    "print(f\"   - Results CSV: {results_path}\")\n",
    "print(f\"   - Model checkpoints: {MODELS_DIR}/crossvit_best_seed*.pth\")\n",
    "print(f\"   - Confusion matrices: {RESULTS_DIR}/crossvit_cm_seed*.png\")\n",
    "\n",
    "if MLFLOW_AVAILABLE:\n",
    "    print(f\"\\nüìä MLFLOW:\")\n",
    "    print(f\"   - View results: mlflow ui ‚Üí http://localhost:5000\")\n",
    "    print(f\"   - Experiment: crossvit-covid19-classification\")\n",
    "    print(f\"   - Total runs logged: {len(all_results)}\")\n",
    "\n",
    "print(f\"\\nüéØ NEXT STEPS:\")\n",
    "print(f\"   1. Train baseline models (notebooks 07-11)\")\n",
    "print(f\"   2. Complete all 30 runs (6 models √ó 5 seeds)\")\n",
    "print(f\"   3. Move to Phase 3: Statistical validation\")\n",
    "print(f\"   4. Compare CrossViT vs baselines with hypothesis testing\")\n",
    "\n",
    "print(f\"\\n‚úÖ CrossViT training complete! 1/6 models done.\")\n",
    "print(\"=\" * 70 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}