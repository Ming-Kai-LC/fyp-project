{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Environment Setup & Verification\n",
    "\n",
    "**Author:** Tan Ming Kai (24PMR12003)  \n",
    "**Date:** 2025-11-09  \n",
    "**Purpose:** Verify GPU, dependencies, and environment configuration for CrossViT COVID-19 FYP\n",
    "\n",
    "**Project:** Multi-Scale Vision Transformer (CrossViT) for COVID-19 Chest X-ray Classification  \n",
    "**Hardware:** RTX 4060 8GB VRAM  \n",
    "**Academic Year:** 2025/26\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "1. ‚úÖ Verify GPU availability and CUDA compatibility\n",
    "2. ‚úÖ Check all required dependencies\n",
    "3. ‚úÖ Test CrossViT model loading from timm\n",
    "4. ‚úÖ Validate dataset paths\n",
    "5. ‚úÖ Set up reproducibility configuration\n",
    "6. ‚úÖ Test memory monitoring utilities\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reproducibility Setup & Critical Imports\n",
    "\n",
    "**CRITICAL:** This section MUST run first in ALL notebooks to ensure reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T05:15:54.099139Z",
     "iopub.status.busy": "2025-11-17T05:15:54.098139Z",
     "iopub.status.idle": "2025-11-17T05:15:57.549731Z",
     "shell.execute_reply": "2025-11-17T05:15:57.549731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Random seeds set to 42 for reproducibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All imports successful!\n",
      "PyTorch version: 2.7.1+cu118\n",
      "OpenCV version: 4.12.0\n",
      "Timm version: 1.0.22\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Environment Setup Notebook for CrossViT COVID-19 FYP\n",
    "Author: Tan Ming Kai (24PMR12003)\n",
    "Purpose: Verify all dependencies and hardware before starting data pipeline\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 1. REPRODUCIBILITY SETUP (ALWAYS FIRST!)\n",
    "# ============================================================================\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"‚úÖ Random seeds set to 42 for reproducibility\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. STANDARD LIBRARY IMPORTS\n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 3. DATA SCIENCE LIBRARIES\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# ============================================================================\n",
    "# 4. COMPUTER VISION LIBRARIES\n",
    "# ============================================================================\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import timm  # For CrossViT model\n",
    "\n",
    "# ============================================================================\n",
    "# 5. PYTORCH & DEEP LEARNING\n",
    "# ============================================================================\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "print(\"\\n‚úÖ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"Timm version: {timm.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU & CUDA Verification\n",
    "\n",
    "**Expected Hardware:** NVIDIA RTX 4060 with 8GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T05:15:57.551743Z",
     "iopub.status.busy": "2025-11-17T05:15:57.551743Z",
     "iopub.status.idle": "2025-11-17T05:15:57.564750Z",
     "shell.execute_reply": "2025-11-17T05:15:57.564750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GPU & CUDA VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "‚úì CUDA Available: True\n",
      "‚úì GPU Name: NVIDIA RTX 6000 Ada Generation\n",
      "‚úì GPU Count: 1\n",
      "‚úì CUDA Version: 11.8\n",
      "‚úì Total VRAM: 51.53 GB\n",
      "‚úì GPU Compute Capability: 8.9\n",
      "\n",
      "üìä Current Memory Status:\n",
      "   - Allocated: 0.0000 GB\n",
      "   - Reserved: 0.0000 GB\n",
      "   - Free: 51.53 GB\n",
      "\n",
      "‚ö†Ô∏è  WARNING: Expected RTX 4060 8GB, but detected NVIDIA RTX 6000 Ada Generation\n",
      "   This may affect batch size and training configuration.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "print(\"=\" * 70)\n",
    "print(\"GPU & CUDA VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\n‚úì CUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    # Get GPU details\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    cuda_version = torch.version.cuda\n",
    "    \n",
    "    print(f\"‚úì GPU Name: {gpu_name}\")\n",
    "    print(f\"‚úì GPU Count: {gpu_count}\")\n",
    "    print(f\"‚úì CUDA Version: {cuda_version}\")\n",
    "    \n",
    "    # Get VRAM information\n",
    "    gpu_properties = torch.cuda.get_device_properties(0)\n",
    "    total_memory_gb = gpu_properties.total_memory / 1e9\n",
    "    \n",
    "    print(f\"‚úì Total VRAM: {total_memory_gb:.2f} GB\")\n",
    "    print(f\"‚úì GPU Compute Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "    \n",
    "    # Check current memory usage\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved_memory = torch.cuda.memory_reserved(0) / 1e9\n",
    "    \n",
    "    print(f\"\\nüìä Current Memory Status:\")\n",
    "    print(f\"   - Allocated: {allocated_memory:.4f} GB\")\n",
    "    print(f\"   - Reserved: {reserved_memory:.4f} GB\")\n",
    "    print(f\"   - Free: {total_memory_gb - reserved_memory:.2f} GB\")\n",
    "    \n",
    "    # Verify expected hardware\n",
    "    if \"4060\" in gpu_name and 7.0 <= total_memory_gb <= 9.0:\n",
    "        print(\"\\n‚úÖ CONFIRMED: RTX 4060 8GB detected - Ready for training!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Expected RTX 4060 8GB, but detected {gpu_name}\")\n",
    "        print(f\"   This may affect batch size and training configuration.\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå ERROR: CUDA not available!\")\n",
    "    print(\"   Please check:\")\n",
    "    print(\"   1. NVIDIA GPU drivers installed\")\n",
    "    print(\"   2. CUDA toolkit installed\")\n",
    "    print(\"   3. PyTorch installed with CUDA support\")\n",
    "    print(\"\\n   Install PyTorch with CUDA:\")\n",
    "    print(\"   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CrossViT Model Loading Test\n",
    "\n",
    "**Objective:** Verify that CrossViT-Tiny can be loaded and run on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T05:15:57.566755Z",
     "iopub.status.busy": "2025-11-17T05:15:57.565744Z",
     "iopub.status.idle": "2025-11-17T05:16:03.691170Z",
     "shell.execute_reply": "2025-11-17T05:16:03.691170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CROSSVIT MODEL LOADING TEST\n",
      "======================================================================\n",
      "\n",
      "‚úì Using device: cuda\n",
      "\n",
      "üì• Loading CrossViT-Tiny from timm library...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bfb34650094ecab9e6c57c1c8bc4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/28.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "\n",
      "üìä Model Statistics:\n",
      "   - Total parameters: 6,725,960\n",
      "   - Trainable parameters: 6,725,960\n",
      "   - Model size: ~26.90 MB (FP32)\n",
      "\n",
      "üß™ Testing forward pass with dummy input...\n",
      "‚úÖ Forward pass successful!\n",
      "   - Input shape: torch.Size([1, 3, 240, 240])\n",
      "   - Output shape: torch.Size([1, 4])\n",
      "   - Expected output shape: torch.Size([1, 4]) for 4 classes\n",
      "\n",
      "‚úÖ Model configuration CORRECT for COVID-19 4-class classification!\n",
      "\n",
      "üìä GPU Memory after model loading: 0.0362 GB\n",
      "   - Estimated memory for batch_size=8: ~0.29 GB\n",
      "   ‚úÖ Should fit comfortably in 8GB VRAM with batch_size=8\n",
      "\n",
      "‚úÖ CrossViT model test PASSED!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CROSSVIT MODEL LOADING TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n‚úì Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Load CrossViT-Tiny model\n",
    "    print(\"\\nüì• Loading CrossViT-Tiny from timm library...\")\n",
    "    model = timm.create_model('crossvit_tiny_240', pretrained=True, num_classes=4)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nüìä Model Statistics:\")\n",
    "    print(f\"   - Total parameters: {total_params:,}\")\n",
    "    print(f\"   - Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   - Model size: ~{total_params * 4 / 1e6:.2f} MB (FP32)\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    print(\"\\nüß™ Testing forward pass with dummy input...\")\n",
    "    dummy_input = torch.randn(1, 3, 240, 240).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "    \n",
    "    print(f\"‚úÖ Forward pass successful!\")\n",
    "    print(f\"   - Input shape: {dummy_input.shape}\")\n",
    "    print(f\"   - Output shape: {output.shape}\")\n",
    "    print(f\"   - Expected output shape: torch.Size([1, 4]) for 4 classes\")\n",
    "    \n",
    "    if output.shape == torch.Size([1, 4]):\n",
    "        print(\"\\n‚úÖ Model configuration CORRECT for COVID-19 4-class classification!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Output shape {output.shape} doesn't match expected [1, 4]\")\n",
    "    \n",
    "    # Check memory usage after loading model\n",
    "    if torch.cuda.is_available():\n",
    "        model_memory = torch.cuda.memory_allocated(0) / 1e9\n",
    "        print(f\"\\nüìä GPU Memory after model loading: {model_memory:.4f} GB\")\n",
    "        print(f\"   - Estimated memory for batch_size=8: ~{model_memory * 8:.2f} GB\")\n",
    "        \n",
    "        if model_memory * 8 < 7.5:  # Safe threshold for 8GB VRAM\n",
    "            print(\"   ‚úÖ Should fit comfortably in 8GB VRAM with batch_size=8\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  May need smaller batch size or gradient accumulation\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model, dummy_input, output\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n‚úÖ CrossViT model test PASSED!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR loading CrossViT model: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Ensure timm is installed: pip install timm\")\n",
    "    print(\"2. Check internet connection (for pretrained weights)\")\n",
    "    print(\"3. Verify sufficient disk space for model download\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Path Verification\n",
    "\n",
    "**Expected:** COVID-19 Radiography Database with 21,165 images across 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T05:16:03.693171Z",
     "iopub.status.busy": "2025-11-17T05:16:03.693171Z",
     "iopub.status.idle": "2025-11-17T05:16:03.846665Z",
     "shell.execute_reply": "2025-11-17T05:16:03.846665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATASET PATH VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "üîç Searching for dataset...\n",
      "   ‚ùå Not found: ..\\data\\raw\\COVID-19_Radiography_Dataset\\COVID-19_Radiography_Dataset\n",
      "‚úÖ Found dataset at: C:\\Users\\FOCS1\\Documents\\GitHub\\fyp-project\\FYP_Code\\notebooks\\..\\data\\raw\\COVID-19_Radiography_Dataset\n",
      "\n",
      "üìÇ Dataset Location: C:\\Users\\FOCS1\\Documents\\GitHub\\fyp-project\\FYP_Code\\notebooks\\..\\data\\raw\\COVID-19_Radiography_Dataset\n",
      "\n",
      "üìä Class Distribution:\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ COVID               :  3616 images (expected ~3616)\n",
      "   Sample: COVID-1.png\n",
      "‚úÖ Normal              : 10192 images (expected ~10192)\n",
      "   Sample: Normal-1.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Lung_Opacity        :  6012 images (expected ~6012)\n",
      "   Sample: Lung_Opacity-1.png\n",
      "‚úÖ Viral Pneumonia     :  1345 images (expected ~1345)\n",
      "   Sample: Viral Pneumonia-1.png\n",
      "----------------------------------------------------------------------\n",
      "   Total Images: 21165 (expected ~21,165)\n",
      "\n",
      "‚úÖ Dataset verification PASSED!\n",
      "‚úÖ Dataset ready for loading in subsequent notebooks\n",
      "\n",
      "üß™ Testing image loading...\n",
      "‚úÖ Successfully loaded: COVID-1.png\n",
      "   - OpenCV shape: (299, 299)\n",
      "   - PIL size: (299, 299)\n",
      "   - PIL mode: L\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DATASET PATH VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define expected dataset structure\n",
    "DATA_ROOT = Path(\"../data/raw/COVID-19_Radiography_Dataset\")\n",
    "\n",
    "# Alternative paths to check\n",
    "alternative_paths = [\n",
    "    Path(\"../data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset\"),  # Nested structure\n",
    "    Path(\"../data/raw/COVID-19_Radiography_Dataset\"),\n",
    "    Path(\"./data/raw/COVID-19_Radiography_Dataset\"),\n",
    "    Path(\"D:/Users/USER/Documents/Visual_Studio_Code/FYP_Code/data/raw/COVID-19_Radiography_Dataset\"),\n",
    "]\n",
    "\n",
    "# Expected classes and approximate counts\n",
    "EXPECTED_CLASSES = {\n",
    "    \"COVID\": 3616,\n",
    "    \"Normal\": 10192,\n",
    "    \"Lung_Opacity\": 6012,\n",
    "    \"Viral Pneumonia\": 1345\n",
    "}\n",
    "\n",
    "dataset_found = False\n",
    "dataset_path = None\n",
    "\n",
    "# Try to find the dataset\n",
    "print(\"\\nüîç Searching for dataset...\")\n",
    "for path in alternative_paths:\n",
    "    if path.exists():\n",
    "        # Check if this path actually contains the class folders\n",
    "        has_classes = any((path / class_name).exists() for class_name in EXPECTED_CLASSES.keys())\n",
    "        if has_classes:\n",
    "            print(f\"‚úÖ Found dataset at: {path.absolute()}\")\n",
    "            dataset_path = path\n",
    "            dataset_found = True\n",
    "            break\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Path exists but no class folders: {path}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Not found: {path}\")\n",
    "\n",
    "if dataset_found:\n",
    "    print(f\"\\nüìÇ Dataset Location: {dataset_path.absolute()}\")\n",
    "    \n",
    "    # Check for class folders\n",
    "    print(f\"\\nüìä Class Distribution:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    total_images = 0\n",
    "    for class_name, expected_count in EXPECTED_CLASSES.items():\n",
    "        class_path = dataset_path / class_name / \"images\"\n",
    "        \n",
    "        if class_path.exists():\n",
    "            image_files = list(class_path.glob(\"*.png\"))\n",
    "            actual_count = len(image_files)\n",
    "            total_images += actual_count\n",
    "            \n",
    "            # Check if count matches expected\n",
    "            match_status = \"‚úÖ\" if abs(actual_count - expected_count) < 100 else \"‚ö†Ô∏è \"\n",
    "            \n",
    "            print(f\"{match_status} {class_name:20s}: {actual_count:5d} images (expected ~{expected_count})\")\n",
    "            \n",
    "            # Show sample image path\n",
    "            if image_files:\n",
    "                print(f\"   Sample: {image_files[0].name}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {class_name:20s}: Folder not found at {class_path}\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(f\"   Total Images: {total_images} (expected ~21,165)\")\n",
    "    \n",
    "    if 20000 <= total_images <= 22000:\n",
    "        print(\"\\n‚úÖ Dataset verification PASSED!\")\n",
    "        print(f\"‚úÖ Dataset ready for loading in subsequent notebooks\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Total image count {total_images} differs from expected ~21,165\")\n",
    "        print(\"   Please verify dataset integrity\")\n",
    "    \n",
    "    # Test loading a single image\n",
    "    print(\"\\nüß™ Testing image loading...\")\n",
    "    try:\n",
    "        # Find first image\n",
    "        for class_name in EXPECTED_CLASSES.keys():\n",
    "            class_path = dataset_path / class_name / \"images\"\n",
    "            if class_path.exists():\n",
    "                image_files = list(class_path.glob(\"*.png\"))\n",
    "                if image_files:\n",
    "                    test_image_path = image_files[0]\n",
    "                    \n",
    "                    # Load with OpenCV\n",
    "                    img_cv = cv2.imread(str(test_image_path), cv2.IMREAD_GRAYSCALE)\n",
    "                    \n",
    "                    # Load with PIL\n",
    "                    img_pil = Image.open(test_image_path)\n",
    "                    \n",
    "                    print(f\"‚úÖ Successfully loaded: {test_image_path.name}\")\n",
    "                    print(f\"   - OpenCV shape: {img_cv.shape}\")\n",
    "                    print(f\"   - PIL size: {img_pil.size}\")\n",
    "                    print(f\"   - PIL mode: {img_pil.mode}\")\n",
    "                    \n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading test image: {e}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå ERROR: Dataset not found!\")\n",
    "    print(\"\\nüìã Setup Instructions:\")\n",
    "    print(\"1. Download COVID-19 Radiography Database from Kaggle:\")\n",
    "    print(\"   https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\")\n",
    "    print(\"\\n2. Extract to: FYP_Code/data/raw/COVID-19_Radiography_Dataset/\")\n",
    "    print(\"\\n3. Expected structure:\")\n",
    "    print(\"   data/raw/COVID-19_Radiography_Dataset/\")\n",
    "    print(\"   ‚îú‚îÄ‚îÄ COVID/images/\")\n",
    "    print(\"   ‚îú‚îÄ‚îÄ Normal/images/\")\n",
    "    print(\"   ‚îú‚îÄ‚îÄ Lung_Opacity/images/\")\n",
    "    print(\"   ‚îî‚îÄ‚îÄ Viral Pneumonia/images/\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Project Configuration (Single Source of Truth)\n",
    "\n",
    "All subsequent notebooks should import this configuration for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Environment Verification Summary\n",
    "\n",
    "### Checklist Status\n",
    "\n",
    "Run this notebook and verify all checks pass before proceeding to data loading:\n",
    "\n",
    "- [ ] ‚úÖ Random seeds set (seed=42)\n",
    "- [ ] ‚úÖ All dependencies imported successfully\n",
    "- [ ] ‚úÖ GPU detected (RTX 4060 8GB VRAM)\n",
    "- [ ] ‚úÖ CUDA available and compatible\n",
    "- [ ] ‚úÖ CrossViT-Tiny model loads correctly\n",
    "- [ ] ‚úÖ Forward pass successful (output shape: [1, 4])\n",
    "- [ ] ‚úÖ Dataset found at expected path\n",
    "- [ ] ‚úÖ All 4 class folders verified (~21,165 images)\n",
    "- [ ] ‚úÖ Sample images load successfully\n",
    "- [ ] ‚úÖ Project configuration created\n",
    "- [ ] ‚úÖ Memory monitoring utilities ready\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Once all checks pass above:\n",
    "\n",
    "1. **Next Notebook:** `01_data_loading.ipynb`\n",
    "   - Load all 21,165 images\n",
    "   - Create train/val/test splits (80/10/10)\n",
    "   - Save image paths to CSV for efficient loading\n",
    "   - Verify class distribution\n",
    "\n",
    "2. **Subsequent Pipeline:**\n",
    "   ```\n",
    "   02_data_cleaning.ipynb    ‚Üí CLAHE enhancement, validation\n",
    "   03_eda.ipynb              ‚Üí Statistical analysis, visualizations\n",
    "   04_data_augmentation.ipynb ‚Üí Test augmentation strategies\n",
    "   05_baseline_models.ipynb   ‚Üí Train 5 baseline models\n",
    "   06_crossvit_training.ipynb ‚Üí Main model training\n",
    "   07_results_analysis.ipynb  ‚Üí Statistical tests, hypothesis validation\n",
    "   08_ablation_studies.ipynb  ‚Üí Test H2, H3, H4 hypotheses\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**If GPU not detected:**\n",
    "- Check NVIDIA drivers: `nvidia-smi`\n",
    "- Reinstall PyTorch with CUDA: `pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118`\n",
    "\n",
    "**If CrossViT fails to load:**\n",
    "- Check internet connection (downloads pretrained weights)\n",
    "- Install timm: `pip install timm`\n",
    "- Verify disk space for model cache (~100MB)\n",
    "\n",
    "**If dataset not found:**\n",
    "- Download from: https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\n",
    "- Extract to: `data/raw/COVID-19_Radiography_Dataset/`\n",
    "\n",
    "---\n",
    "\n",
    "**‚úÖ Environment setup complete! Ready to begin FYP implementation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T05:16:03.848668Z",
     "iopub.status.busy": "2025-11-17T05:16:03.848668Z",
     "iopub.status.idle": "2025-11-17T05:16:03.957624Z",
     "shell.execute_reply": "2025-11-17T05:16:03.956915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MEMORY MONITORING UTILITIES\n",
      "======================================================================\n",
      "\n",
      "‚úì GPU memory utilities loaded:\n",
      "  - print_gpu_memory(prefix): Print current memory usage\n",
      "  - clear_memory(): Clear GPU cache\n",
      "  - GPUMemoryMonitor(name): Context manager for memory tracking\n",
      "\n",
      "üß™ Testing memory monitor...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Starting: Test Operation\n",
      "  Before: GPU Memory: Allocated=0.009GB | Reserved=0.021GB | Free=51.506GB\n",
      "  After:  GPU Memory: Allocated=0.009GB | Reserved=0.021GB | Free=51.506GB\n",
      "  Memory change: +0.000GB\n",
      "Completed: Test Operation\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Memory monitoring utilities ready!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def print_gpu_memory(prefix=\"\"):\n",
    "    \"\"\"Print current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        free = total - reserved\n",
    "        \n",
    "        print(f\"{prefix}GPU Memory: Allocated={allocated:.3f}GB | Reserved={reserved:.3f}GB | Free={free:.3f}GB\")\n",
    "    else:\n",
    "        print(f\"{prefix}GPU not available\")\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU cache and run garbage collection.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "class GPUMemoryMonitor:\n",
    "    \"\"\"Context manager for monitoring GPU memory during operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, operation_name=\"Operation\"):\n",
    "        self.operation_name = operation_name\n",
    "        self.start_memory = 0\n",
    "        \n",
    "    def __enter__(self):\n",
    "        clear_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            self.start_memory = torch.cuda.memory_allocated(0) / 1e9\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Starting: {self.operation_name}\")\n",
    "        print_gpu_memory(\"  Before: \")\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if torch.cuda.is_available():\n",
    "            end_memory = torch.cuda.memory_allocated(0) / 1e9\n",
    "            delta = end_memory - self.start_memory\n",
    "            print_gpu_memory(\"  After:  \")\n",
    "            print(f\"  Memory change: {delta:+.3f}GB\")\n",
    "        print(f\"Completed: {self.operation_name}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "\n",
    "\n",
    "# Test the utilities\n",
    "print(\"=\" * 70)\n",
    "print(\"MEMORY MONITORING UTILITIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚úì GPU memory utilities loaded:\")\n",
    "print(\"  - print_gpu_memory(prefix): Print current memory usage\")\n",
    "print(\"  - clear_memory(): Clear GPU cache\")\n",
    "print(\"  - GPUMemoryMonitor(name): Context manager for memory tracking\")\n",
    "\n",
    "print(\"\\nüß™ Testing memory monitor...\")\n",
    "test_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "with GPUMemoryMonitor(\"Test Operation\"):\n",
    "    # Create a small tensor to demonstrate\n",
    "    if torch.cuda.is_available():\n",
    "        test_tensor = torch.randn(1000, 1000).to(test_device)\n",
    "        del test_tensor\n",
    "\n",
    "print(\"‚úÖ Memory monitoring utilities ready!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Memory Monitoring Utilities\n",
    "\n",
    "Helper functions for tracking GPU memory during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T05:16:03.957624Z",
     "iopub.status.busy": "2025-11-17T05:16:03.957624Z",
     "iopub.status.idle": "2025-11-17T05:16:03.987650Z",
     "shell.execute_reply": "2025-11-17T05:16:03.986427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROJECT CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "‚úì Device: cuda\n",
      "‚úì Batch size: 8 (effective: 32)\n",
      "‚úì Image size: 240√ó240\n",
      "‚úì Number of classes: 4\n",
      "‚úì Learning rate: 5e-05\n",
      "‚úì Max epochs: 50\n",
      "‚úì Mixed precision: True\n",
      "\n",
      "‚úÖ Configuration ready!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PROJECT CONFIGURATION - Single Source of Truth\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Reproducibility\n",
    "    'seed': 42,\n",
    "    \n",
    "    # Hardware\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Data paths\n",
    "    'data_root': Path('../data/raw/COVID-19_Radiography_Dataset'),\n",
    "    'processed_data_dir': Path('../data/processed'),\n",
    "    'output_dir': Path('../results'),\n",
    "    'models_dir': Path('../models'),\n",
    "    \n",
    "    # Dataset specifications\n",
    "    'image_size': 240,  # CrossViT-Tiny requirement\n",
    "    'num_classes': 4,\n",
    "    'class_names': ['COVID', 'Normal', 'Lung_Opacity', 'Viral Pneumonia'],\n",
    "    'class_weights': [1.47, 0.52, 0.88, 3.95],  # For class imbalance\n",
    "    \n",
    "    # Data split\n",
    "    'train_ratio': 0.8,\n",
    "    'val_ratio': 0.1,\n",
    "    'test_ratio': 0.1,\n",
    "    \n",
    "    # CLAHE parameters (for preprocessing)\n",
    "    'clahe_clip_limit': 2.0,\n",
    "    'clahe_tile_grid_size': (8, 8),\n",
    "    \n",
    "    # ImageNet normalization (required for pretrained models)\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225],\n",
    "    \n",
    "    # Training hyperparameters (memory-safe for RTX 4060 8GB)\n",
    "    'batch_size': 8,\n",
    "    'gradient_accumulation_steps': 4,  # Effective batch size = 32\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True,\n",
    "    'persistent_workers': True,\n",
    "    \n",
    "    # Optimizer settings\n",
    "    'learning_rate': 5e-5,\n",
    "    'weight_decay': 0.05,\n",
    "    'max_epochs': 50,\n",
    "    'early_stopping_patience': 15,\n",
    "    \n",
    "    # Mixed precision training\n",
    "    'use_mixed_precision': True,\n",
    "    \n",
    "    # Logging\n",
    "    'log_interval': 50,  # Print every 50 batches\n",
    "    'save_interval': 5,  # Save checkpoint every 5 epochs\n",
    "}\n",
    "\n",
    "# Create necessary directories\n",
    "CONFIG['processed_data_dir'].mkdir(parents=True, exist_ok=True)\n",
    "CONFIG['output_dir'].mkdir(parents=True, exist_ok=True)\n",
    "CONFIG['models_dir'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROJECT CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n‚úì Device: {CONFIG['device']}\")\n",
    "print(f\"‚úì Batch size: {CONFIG['batch_size']} (effective: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']})\")\n",
    "print(f\"‚úì Image size: {CONFIG['image_size']}√ó{CONFIG['image_size']}\")\n",
    "print(f\"‚úì Number of classes: {CONFIG['num_classes']}\")\n",
    "print(f\"‚úì Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"‚úì Max epochs: {CONFIG['max_epochs']}\")\n",
    "print(f\"‚úì Mixed precision: {CONFIG['use_mixed_precision']}\")\n",
    "print(\"\\n‚úÖ Configuration ready!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP_Code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1fe0cf2c031747e4ac4697f40d051ae4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "242699dd3a4c40f9b5c528c438182bc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "27ca33ce0a6043a799799a4c15d07aec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "47536d6fe76446aa849282e31d6df5bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8faec51e4295431d929ff968586bf421",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_4aff1ac48b5747259b14ca7bb89c767d",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors:‚Äá100%"
      }
     },
     "4aff1ac48b5747259b14ca7bb89c767d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8faec51e4295431d929ff968586bf421": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "99bfb34650094ecab9e6c57c1c8bc4ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_47536d6fe76446aa849282e31d6df5bd",
        "IPY_MODEL_fb110e371b7f490cb037db938149afd5",
        "IPY_MODEL_ab96db781db84155b0d3ef7a548991cb"
       ],
       "layout": "IPY_MODEL_27ca33ce0a6043a799799a4c15d07aec",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ab96db781db84155b0d3ef7a548991cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c82a571308c74395832f7ec33e6c798d",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_1fe0cf2c031747e4ac4697f40d051ae4",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá28.1M/28.1M‚Äá[00:04&lt;00:00,‚Äá5.75MB/s]"
      }
     },
     "b1ae26d740c84c788851a2055ad74577": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c82a571308c74395832f7ec33e6c798d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fb110e371b7f490cb037db938149afd5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b1ae26d740c84c788851a2055ad74577",
       "max": 28089260.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_242699dd3a4c40f9b5c528c438182bc7",
       "tabbable": null,
       "tooltip": null,
       "value": 28089260.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
