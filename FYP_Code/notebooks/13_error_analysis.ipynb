{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 - Error Analysis (Phase 3)\n",
    "\n",
    "**Author:** Tan Ming Kai (24PMR12003)  \n",
    "**Date:** 2025-11-24  \n",
    "**Purpose:** Deep analysis of misclassifications for thesis Chapter 5\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. **Per-class performance breakdown** (Precision, Recall, F1, Specificity)\n",
    "2. **Confusion matrix analysis** (which classes confused?)\n",
    "3. **Medical metrics calculation** (Sensitivity, Specificity, PPV, NPV)\n",
    "4. **Misclassification visualization** (visualize failed cases)\n",
    "5. **Pattern identification** (common failure modes)\n",
    "\n",
    "---\n",
    "\n",
    "## Why Error Analysis?\n",
    "\n",
    "Understanding **where and why** models fail is crucial for:\n",
    "- Thesis discussion (Chapter 6)\n",
    "- Clinical deployment considerations\n",
    "- Future improvement directions\n",
    "- Identifying systematic biases\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os, sys, warnings, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, auc, roc_auc_score\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"[OK] Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "MODELS_DIR = Path(\"../experiments/phase2_systematic/models\")\n",
    "OUTPUT_DIR = Path(\"../experiments/phase3_analysis/error_analysis\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLASS_NAMES = ['COVID', 'Normal', 'Lung_Opacity', 'Viral_Pneumonia']\n",
    "\n",
    "CONFIG = {\n",
    "    'seed': 42,  # Analyze best seed for each model\n",
    "    'batch_size': 16,\n",
    "    'num_workers': 0,\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225]\n",
    "}\n",
    "\n",
    "print(f\"[OK] Configuration set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class COVID19Dataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.image_paths = dataframe['processed_path'].values\n",
    "        self.labels = dataframe['label'].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return image, label, self.image_paths[idx]\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv(DATA_DIR / \"test_processed.csv\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(f\"Class distribution:\\n{test_df['label'].value_counts().sort_index()}\")\n",
    "\n",
    "# Transform (no augmentation for test)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=CONFIG['mean'], std=CONFIG['std'])\n",
    "])\n",
    "\n",
    "test_dataset = COVID19Dataset(test_df, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], \n",
    "                         shuffle=False, num_workers=CONFIG['num_workers'])\n",
    "\n",
    "print(\"[OK] Test data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Predictions from Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, loader, device):\n",
    "    \"\"\"\n",
    "    Get predictions, true labels, and image paths.\n",
    "    \n",
    "    Returns:\n",
    "        predictions, true_labels, image_paths, probabilities\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths in tqdm(loader, desc=\"Getting predictions\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_paths.extend(paths)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), all_paths, np.array(all_probs)\n",
    "\n",
    "# Load best models (seed 42 for consistency)\n",
    "models_to_analyze = {\n",
    "    'CrossViT-Tiny': {\n",
    "        'path': MODELS_DIR / 'crossvit' / 'crossvit_best_seed42.pth',\n",
    "        'model_fn': lambda: timm.create_model('crossvit_tiny_240', pretrained=False, num_classes=4)\n",
    "    },\n",
    "    'ResNet-50': {\n",
    "        'path': MODELS_DIR / 'resnet50' / 'resnet50_best_seed42.pth',\n",
    "        'model_fn': lambda: timm.create_model('resnet50', pretrained=False, num_classes=4)\n",
    "    },\n",
    "    'DenseNet-121': {\n",
    "        'path': MODELS_DIR / 'densenet121' / 'densenet121_best_seed42.pth',\n",
    "        'model_fn': lambda: timm.create_model('densenet121', pretrained=False, num_classes=4)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get predictions for all models\n",
    "predictions_dict = {}\n",
    "\n",
    "for model_name, model_info in models_to_analyze.items():\n",
    "    print(f\"\\nLoading {model_name}...\")\n",
    "    model = model_info['model_fn']()\n",
    "    model.load_state_dict(torch.load(model_info['path'], map_location=device))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    preds, labels, paths, probs = get_predictions(model, test_loader, device)\n",
    "    \n",
    "    predictions_dict[model_name] = {\n",
    "        'predictions': preds,\n",
    "        'true_labels': labels,\n",
    "        'image_paths': paths,\n",
    "        'probabilities': probs\n",
    "    }\n",
    "    \n",
    "    acc = accuracy_score(labels, preds) * 100\n",
    "    print(f\"[OK] {model_name} accuracy: {acc:.2f}%\")\n",
    "\n",
    "print(\"\\n[OK] All predictions obtained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics for each model\n",
    "print(\"\\nPER-CLASS PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for model_name, pred_data in predictions_dict.items():\n",
    "    print(f\"\\n{model_name}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    preds = pred_data['predictions']\n",
    "    labels = pred_data['true_labels']\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(labels, preds, target_names=CLASS_NAMES, \n",
    "                                   output_dict=True, zero_division=0)\n",
    "    \n",
    "    print(f\"{'Class':<20s} {'Precision':<12s} {'Recall':<12s} {'F1-Score':<12s} {'Support'}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for class_name in CLASS_NAMES:\n",
    "        metrics = report[class_name]\n",
    "        print(f\"{class_name:<20s} {metrics['precision']:>6.2f}{'':6s} \"\n",
    "              f\"{metrics['recall']:>6.2f}{'':6s} {metrics['f1-score']:>6.2f}{'':6s} \"\n",
    "              f\"{int(metrics['support']):>5d}\")\n",
    "    \n",
    "    # Overall\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Overall Accuracy':<20s} {report['accuracy']:.4f}\")\n",
    "    print(f\"{'Macro Avg F1':<20s} {report['macro avg']['f1-score']:.4f}\")\n",
    "    print(f\"{'Weighted Avg F1':<20s} {report['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "print(\"\\n[OK] Per-class analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices side-by-side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (model_name, pred_data) in enumerate(predictions_dict.items()):\n",
    "    cm = confusion_matrix(pred_data['true_labels'], pred_data['predictions'])\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(cm_normalized, annot=cm, fmt='d', cmap='Blues', \n",
    "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "                cbar=True, ax=axes[idx], vmin=0, vmax=1)\n",
    "    \n",
    "    axes[idx].set_ylabel('True Label', fontsize=11)\n",
    "    axes[idx].set_xlabel('Predicted Label', fontsize=11)\n",
    "    axes[idx].set_title(model_name, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'confusion_matrices_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"[OK] Confusion matrices plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Medical Metrics (Sensitivity, Specificity, PPV, NPV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_medical_metrics(cm, class_idx):\n",
    "    \"\"\"\n",
    "    Calculate medical metrics for binary classification (one-vs-rest).\n",
    "    \n",
    "    Args:\n",
    "        cm: Confusion matrix (multi-class)\n",
    "        class_idx: Index of the class to calculate metrics for\n",
    "    \n",
    "    Returns:\n",
    "        sensitivity, specificity, ppv, npv\n",
    "    \"\"\"\n",
    "    # Convert to binary (one-vs-rest)\n",
    "    tp = cm[class_idx, class_idx]\n",
    "    fn = cm[class_idx, :].sum() - tp\n",
    "    fp = cm[:, class_idx].sum() - tp\n",
    "    tn = cm.sum() - tp - fn - fp\n",
    "    \n",
    "    # Medical metrics\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Recall\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Precision\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    \n",
    "    return sensitivity, specificity, ppv, npv\n",
    "\n",
    "# Calculate for COVID class (most critical)\n",
    "print(\"\\nMEDICAL METRICS FOR COVID CLASS (One-vs-Rest)\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Model':<20s} {'Sensitivity':<15s} {'Specificity':<15s} {'PPV':<15s} {'NPV'}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "covid_idx = 0  # COVID is class 0\n",
    "\n",
    "for model_name, pred_data in predictions_dict.items():\n",
    "    cm = confusion_matrix(pred_data['true_labels'], pred_data['predictions'])\n",
    "    sens, spec, ppv, npv = calculate_medical_metrics(cm, covid_idx)\n",
    "    \n",
    "    print(f\"{model_name:<20s} {sens:>6.2%}{'':9s} {spec:>6.2%}{'':9s} \"\n",
    "          f\"{ppv:>6.2%}{'':9s} {npv:>6.2%}\")\n",
    "\n",
    "print(\"\\nNote: Sensitivity = Recall, PPV = Precision\")\n",
    "print(\"High sensitivity crucial for COVID detection (minimize false negatives)\")\n",
    "print(\"High specificity crucial to avoid false alarms (minimize false positives)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Identify Misclassified Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified samples for CrossViT\n",
    "crossvit_data = predictions_dict['CrossViT-Tiny']\n",
    "preds = crossvit_data['predictions']\n",
    "labels = crossvit_data['true_labels']\n",
    "paths = crossvit_data['image_paths']\n",
    "probs = crossvit_data['probabilities']\n",
    "\n",
    "# Find misclassifications\n",
    "misclassified_mask = preds != labels\n",
    "misclassified_indices = np.where(misclassified_mask)[0]\n",
    "\n",
    "print(f\"\\nTotal misclassifications: {len(misclassified_indices)} / {len(labels)} \"\n",
    "      f\"({len(misclassified_indices)/len(labels)*100:.2f}%)\")\n",
    "\n",
    "# Create misclassification dataframe\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'image_path': [paths[i] for i in misclassified_indices],\n",
    "    'true_label': [CLASS_NAMES[labels[i]] for i in misclassified_indices],\n",
    "    'predicted_label': [CLASS_NAMES[preds[i]] for i in misclassified_indices],\n",
    "    'confidence': [probs[i, preds[i]] for i in misclassified_indices]\n",
    "})\n",
    "\n",
    "# Sort by confidence (high confidence errors are interesting)\n",
    "misclassified_df = misclassified_df.sort_values('confidence', ascending=False)\n",
    "\n",
    "# Save\n",
    "misclassified_df.to_csv(OUTPUT_DIR / 'misclassified_cases.csv', index=False)\n",
    "print(f\"[OK] Misclassification details saved\")\n",
    "\n",
    "# Show top 10 high-confidence errors\n",
    "print(\"\\nTop 10 High-Confidence Misclassifications:\")\n",
    "print(misclassified_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Misclassified Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 12 random misclassified samples\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(misclassified_indices, size=min(12, len(misclassified_indices)), replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices):\n",
    "    img_path = paths[sample_idx]\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    true_label = CLASS_NAMES[labels[sample_idx]]\n",
    "    pred_label = CLASS_NAMES[preds[sample_idx]]\n",
    "    confidence = probs[sample_idx, preds[sample_idx]]\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f\"True: {true_label}\\nPred: {pred_label} ({confidence:.1%})\", \n",
    "                       fontsize=10, color='red')\n",
    "\n",
    "plt.suptitle('Misclassified Samples (CrossViT-Tiny)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'misclassified_samples_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"[OK] Misclassification visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Patterns Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which class pairs are most confused\n",
    "print(\"\\nERROR PATTERN ANALYSIS: Which classes are most confused?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cm = confusion_matrix(labels, preds)\n",
    "\n",
    "# Find top confusions (off-diagonal)\n",
    "confusions = []\n",
    "for i in range(len(CLASS_NAMES)):\n",
    "    for j in range(len(CLASS_NAMES)):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confusions.append({\n",
    "                'True': CLASS_NAMES[i],\n",
    "                'Predicted': CLASS_NAMES[j],\n",
    "                'Count': cm[i, j],\n",
    "                'Percentage': cm[i, j] / cm[i, :].sum() * 100\n",
    "            })\n",
    "\n",
    "confusion_df = pd.DataFrame(confusions).sort_values('Count', ascending=False)\n",
    "\n",
    "print(f\"{'True Label':<20s} {'Predicted As':<20s} {'Count':<10s} {'% of True Class'}\")\n",
    "print(\"-\"*80)\n",
    "for _, row in confusion_df.head(10).iterrows():\n",
    "    print(f\"{row['True']:<20s} {row['Predicted']:<20s} {row['Count']:<10.0f} {row['Percentage']:.2f}%\")\n",
    "\n",
    "print(\"\\n[OK] Error pattern analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary for Thesis\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Overall Performance:**\n",
    "   - CrossViT-Tiny: X.XX% accuracy\n",
    "   - Misclassified XX / XXXX samples (X.XX%)\n",
    "\n",
    "2. **Per-Class Performance:**\n",
    "   - **Best:** [Class with highest F1]\n",
    "   - **Worst:** [Class with lowest F1]\n",
    "   - **COVID Detection:** Sensitivity = X.XX%, Specificity = X.XX%\n",
    "\n",
    "3. **Common Errors:**\n",
    "   - Most frequent confusion: [Class A] â†’ [Class B]\n",
    "   - High-confidence errors: XX cases (may indicate systematic bias)\n",
    "\n",
    "4. **Clinical Implications:**\n",
    "   - False negatives (COVID missed): XX cases - **HIGH RISK**\n",
    "   - False positives (COVID over-diagnosed): XX cases - Lower risk\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ERROR ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"1. confusion_matrices_comparison.png - Side-by-side confusion matrices\")\n",
    "print(\"2. misclassified_cases.csv - Detailed list of all errors\")\n",
    "print(\"3. misclassified_samples_visualization.png - Visual inspection of failures\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review misclassified cases for patterns\")\n",
    "print(\"2. Discuss clinical implications in thesis Chapter 6\")\n",
    "print(\"3. Proceed to 14_ablation_studies.ipynb for hypothesis testing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
