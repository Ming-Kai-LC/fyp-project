
# Chapter 5: Results Reporting Template

**How to Use This Template:**
1. Copy sections into your thesis Chapter 5
2. Replace [PLACEHOLDERS] with actual values from Phase 3 results
3. Cite figures/tables correctly (Table 5.1, Figure 5.1, etc.)
4. Use APA 7th Edition reporting style

---

## 5.1 Descriptive Statistics

### 5.1.1 Model Performance Overview

All six models were trained with five different random seeds (42, 123, 456, 789, 101112) to assess reproducibility. Table 5.1 presents the mean test accuracy, standard deviation, and 95% bootstrap confidence intervals for each model.

**[Table 5.1 goes here - Copy from experiments/phase3_analysis/statistical_validation/summary_statistics_table.csv]**

*Table 5.1.* Descriptive Statistics for Model Performance Across 5 Random Seeds

| Model | Mean Acc (%) | SD (%) | 95% CI Lower | 95% CI Upper | Rank |
|-------|--------------|--------|--------------|--------------|------|
| ResNet-50 | 95.45 | 0.39 | 94.92 | 96.02 | 1 |
| DenseNet-121 | 95.45 | 0.39 | 94.92 | 96.02 | 1 |
| EfficientNet-B0 | 95.17 | 0.16 | 94.95 | 95.39 | 3 |
| Swin-Tiny | 95.02 | 0.24 | 94.69 | 95.36 | 4 |
| CrossViT-Tiny | 94.96 | 0.40 | 94.40 | 95.50 | 5 |
| ViT-Tiny | 87.98 | 0.44 | 87.36 | 88.54 | 6 |

*Note.* Acc = Accuracy, SD = Standard Deviation, CI = Confidence Interval (bootstrap with 10,000 resamples). N = 5 seeds per model. Test set size = 2,117 images.

**Interpretation:**

All models except ViT-Tiny achieved test accuracy exceeding 94%, indicating strong generalization performance. ResNet-50 and DenseNet-121 tied for the highest mean accuracy (95.45%, 95% CI [94.92, 96.02]), followed closely by EfficientNet-B0 (95.17%) and Swin-Tiny (95.02%). CrossViT-Tiny ranked fifth with 94.96% accuracy (95% CI [94.40, 95.50]), with a standard deviation of 0.40% across seeds, suggesting moderate variability. ViT-Tiny substantially underperformed (87.98%), likely due to its single-scale patch embedding design lacking multi-scale feature extraction.

The narrow confidence intervals (widths < 1.5%) indicate precise estimates despite small sample size (n=5). All models demonstrated low variance across random seeds (SD < 0.5% for top 5 models), confirming reproducible training.

### 5.1.2 Training Characteristics

**[Optional: Add training time, convergence epochs, memory usage]**

Mean training time per epoch ranged from [X minutes] (EfficientNet-B0) to [Y minutes] (Swin-Tiny). All models converged within [Z] epochs on average, with early stopping triggered when validation loss plateaued for 15 consecutive epochs. GPU memory consumption peaked at [A GB] for CrossViT-Tiny and [B GB] for ViT-Tiny, remaining within the 8GB VRAM constraint.

---

## 5.2 Hypothesis Testing

### 5.2.1 H1: CrossViT vs CNN Baselines

**Hypothesis:** CrossViT-Tiny achieves significantly higher test accuracy than CNN baselines (ResNet-50, DenseNet-121, EfficientNet-B0, Swin-Tiny).

**Statistical Test:** Paired t-test with Bonferroni correction (alpha' = 0.01 for 5 comparisons).

**[Table 5.2 goes here - Copy from experiments/phase3_analysis/statistical_validation/hypothesis_testing_results.csv]**

*Table 5.2.* Hypothesis Testing Results: CrossViT-Tiny vs Baseline Models

| Comparison | Mean Diff (%) | t-statistic | p-value | Cohen's d | Significant (alpha'=0.01)? |
|------------|---------------|-------------|---------|-----------|----------------------------|
| CrossViT vs ResNet-50 | -0.49 | -1.266 | 0.277 | -1.134 | No |
| CrossViT vs DenseNet-121 | -0.49 | -1.266 | 0.277 | -1.134 | No |
| CrossViT vs EfficientNet-B0 | -0.21 | -0.821 | 0.459 | -0.702 | No |
| CrossViT vs Swin-Tiny | -0.06 | -0.187 | 0.861 | -0.155 | No |

*Note.* Diff = Difference (CrossViT - Baseline), negative values indicate CrossViT underperformed. Degrees of freedom = 4. Bonferroni-corrected significance threshold: p < 0.01.

**Result:** H1 is **NOT SUPPORTED**. CrossViT-Tiny did not significantly outperform any CNN baseline after Bonferroni correction (all p > 0.01). In fact, ResNet-50 and DenseNet-121 achieved 0.49% higher accuracy than CrossViT (95.45% vs 94.96%), though this difference was not statistically significant (p = 0.277). Effect sizes were large (|d| > 0.8) for ResNet/DenseNet comparisons but failed to reach significance due to overlapping confidence intervals.

**Interpretation:** The lack of significant differences suggests that on this dataset, architectural innovations (dual-branch attention, multi-scale processing) provided no measurable advantage over traditional CNNs. All models converged to similar performance levels (~95% accuracy), indicating that dataset size, preprocessing quality, and class imbalance mitigation may be more critical factors than architecture choice for this classification task.

### 5.2.2 H2: Dual-Branch vs Single-Scale Architecture

**Hypothesis:** CrossViT-Tiny (dual-branch) achieves at least 5% higher accuracy than ViT-Tiny (single-scale).

**Statistical Test:** Paired t-test (alpha = 0.05, one-tailed).

**Result:** H2 is **SUPPORTED**. CrossViT-Tiny achieved 6.98% higher accuracy than ViT-Tiny (94.96% vs 87.98%, p < 0.001, Cohen's d = 4.989). This difference is both statistically significant (p = 0.0003) and practically meaningful (large effect size, d > 0.8).

**[Figure 5.1 goes here - Copy from experiments/phase3_analysis/ablation_studies/h2_dual_branch_analysis.png]**

*Figure 5.1.* Accuracy comparison between CrossViT-Tiny (dual-branch) and ViT-Tiny (single-scale) across 5 random seeds. Error bars represent +/- 1 standard deviation. The dual-branch architecture significantly outperformed single-scale (p < 0.001).

**Interpretation:** The substantial performance gap validates the core contribution of CrossViT's dual-branch design. By processing images at two different patch sizes (16x16 and 12x12) and fusing features via cross-attention, CrossViT captures both coarse global context and fine local details. In contrast, ViT-Tiny's single patch size (16x16) may miss fine-grained pathological features critical for distinguishing between similar lung diseases (e.g., COVID-19 vs Viral Pneumonia). This finding aligns with Chen et al. (2021), who demonstrated that multi-scale representations improve medical image classification.

### 5.2.3 H3 and H4: Ablation Studies (Not Tested)

**H3 (CLAHE Impact):** Testing CLAHE vs no preprocessing requires retraining all models without CLAHE enhancement, which was not feasible within the project timeline.

**H4 (Augmentation Strategy):** Comparing conservative vs aggressive augmentation requires additional systematic experiments (Phase 2 used fixed augmentation).

**Status:** Both hypotheses remain **untested** and are recommended for future work (see Chapter 6, Section 6.4).

---

## 5.3 Medical Performance Metrics

### 5.3.1 COVID-19 Detection Performance

Table 5.3 presents medical evaluation metrics for COVID-19 detection, treating COVID as the positive class and all other diagnoses (Normal, Lung Opacity, Viral Pneumonia) as the negative class.

**[Table 5.3 goes here - Copy from ERROR_ANALYSIS_FINDINGS.md]**

*Table 5.3.* Medical Performance Metrics for COVID-19 Detection

| Model | Sensitivity (%) | Specificity (%) | PPV (%) | NPV (%) | F1-Score (%) |
|-------|-----------------|-----------------|---------|---------|--------------|
| ResNet-50 | 95.44 | 98.43 | 92.62 | 99.05 | 94.01 |
| DenseNet-121 | 95.44 | 98.43 | 92.62 | 99.05 | 94.01 |
| CrossViT-Tiny | 94.88 | 98.23 | 91.71 | 98.94 | 93.27 |

*Note.* Sensitivity = True Positive Rate (Recall), Specificity = True Negative Rate, PPV = Positive Predictive Value (Precision), NPV = Negative Predictive Value. N_COVID = 723, N_Non-COVID = 1,394 (test set).

**Interpretation:**

All three top-performing models demonstrated excellent COVID-19 detection capability. ResNet-50 and DenseNet-121 achieved 95.44% sensitivity, meaning only 33 out of 723 COVID cases (4.56%) were missed. Specificity exceeded 98% for all models, indicating fewer than 25 false positives out of 1,394 non-COVID cases. High negative predictive value (NPV > 98.9%) confirms that negative predictions are highly reliable for ruling out COVID-19.

**Clinical Significance:** A sensitivity of 95% is considered acceptable for screening tools (WHO recommends >80% for rapid diagnostic tests). The low false negative rate (4.56%) minimizes the risk of releasing infected patients into the community. High specificity (>98%) reduces unnecessary isolation and treatment costs. These metrics suggest the models are suitable for preliminary COVID-19 screening in clinical workflows, though confirmatory RT-PCR testing remains the gold standard.

### 5.3.2 Per-Class Performance Analysis

**[Figure 5.2 goes here - Copy from experiments/phase3_analysis/error_analysis/per_class_f1_comparison.png]**

*Figure 5.2.* Per-class F1-scores for CrossViT-Tiny, ResNet-50, and DenseNet-121. All models achieved >96% F1 for Normal class but struggled with Viral Pneumonia (F1 ~ 82-84%) due to class imbalance (only 269 test samples).

**Analysis by Class:**

1. **COVID-19 (N=723):** F1-scores ranged from 93.27% (CrossViT) to 94.01% (ResNet/DenseNet). High precision (>91%) and recall (>94%) indicate strong performance.

2. **Normal (N=2,039):** Best performance across all models (F1 > 96%). The largest class benefits from abundant training examples and distinct radiographic features (absence of pathology).

3. **Lung Opacity (N=1,201):** F1-scores exceeded 95% for all models. Moderate class size and clear consolidation patterns facilitate accurate classification.

4. **Viral Pneumonia (N=269):** Lowest F1-scores (82-84%) despite high recall (>94%). Low precision (72-75%) indicates many false positives. This class represents only 6.4% of the dataset, causing class imbalance issues. Additionally, radiographic overlap with COVID-19 and Lung Opacity complicates differentiation.

**Recommendation:** Collect additional Viral Pneumonia samples or apply targeted data augmentation (e.g., SMOTE, mixup) to address class imbalance in future iterations.

---

## 5.4 Error Analysis

### 5.4.1 Confusion Matrix Analysis

**[Figure 5.3 goes here - Copy from experiments/phase3_analysis/error_analysis/confusion_matrices_comparison.png]**

*Figure 5.3.* Confusion matrices for (a) CrossViT-Tiny, (b) ResNet-50, and (c) DenseNet-121. Diagonal elements represent correct predictions. Off-diagonal elements indicate misclassifications.

**Most Common Misclassifications (CrossViT-Tiny):**

1. **Normal â†’ Viral Pneumonia:** 49 cases (2.40% of Normal)
   - Possible causes: Subtle infiltrates misinterpreted as viral infection
   - Clinical impact: Low risk (both conditions require monitoring)

2. **Normal â†’ COVID-19:** 36 cases (1.77% of Normal)
   - Possible causes: Borderline abnormalities or early-stage infection
   - Clinical impact: Moderate risk (false alarms lead to unnecessary quarantine)

3. **Lung Opacity â†’ Viral Pneumonia:** 30 cases (2.50% of Lung Opacity)
   - Possible causes: Similar radiographic patterns (bilateral infiltrates)
   - Clinical impact: Low risk (both pneumonia types require treatment)

4. **COVID-19 â†’ Normal:** 33 cases (4.56% of COVID)
   - Possible causes: Mild/asymptomatic cases with minimal radiographic findings
   - Clinical impact: **HIGH RISK** (infected patients released into community)

**Mitigation Strategies:**
- Increase sensitivity threshold for COVID detection (trade specificity for sensitivity)
- Ensemble multiple models to reduce false negatives
- Combine AI predictions with clinical symptoms and RT-PCR results

### 5.4.2 Error Patterns by Severity

**[Optional: Analyze if errors correlate with disease severity, patient age, image quality]**

Qualitative analysis of misclassified images suggests that errors concentrate in:
- **Borderline cases:** Subtle or early-stage abnormalities
- **Image quality issues:** Low contrast, motion artifacts, poor positioning
- **Atypical presentations:** Unilateral COVID-19, lobar Viral Pneumonia

**Recommendation:** Implement image quality checks and uncertainty quantification (e.g., Monte Carlo dropout, ensemble disagreement) to flag low-confidence predictions for manual review.

---

## 5.5 Summary of Key Findings

1. **Overall Performance:** All models achieved >94% accuracy, with ResNet-50 and DenseNet-121 tying for best performance (95.45%).

2. **Hypothesis Testing:**
   - **H1 (CrossViT > CNNs):** NOT SUPPORTED - No significant differences detected (p > 0.01)
   - **H2 (Dual-branch > Single-scale):** SUPPORTED - CrossViT outperformed ViT by 6.98% (p < 0.001, d = 4.99)
   - **H3, H4:** Untested due to time constraints

3. **Medical Metrics:** Excellent COVID-19 detection (Sensitivity = 95.44%, Specificity = 98.43%, NPV = 99.05%)

4. **Error Analysis:** Viral Pneumonia most challenging class (F1 = 82-84%) due to class imbalance and radiographic overlap

5. **Clinical Viability:** Models suitable for preliminary COVID-19 screening but require confirmatory testing

**Practical Significance:** Despite lack of statistical difference (H1), all models achieved clinically acceptable performance (>94% accuracy, >95% COVID sensitivity). The choice of model should prioritize inference speed, memory efficiency, and interpretability over marginal accuracy gains (<1%).

---

## 5.6 Reporting Checklist (APA Style)

When writing Chapter 5, ensure you include:

âœ… **Descriptive Statistics:** Mean, SD, 95% CI for all models (Table 5.1)
âœ… **Sample Sizes:** N=5 seeds, N=2,117 test images
âœ… **Statistical Tests:** Test name, assumptions, alpha level, corrections applied
âœ… **Test Statistics:** t-value, p-value, degrees of freedom, effect size (d)
âœ… **Confidence Intervals:** Report for all point estimates
âœ… **Figures:** High-resolution (300 DPI), properly captioned, referenced in text
âœ… **Tables:** APA format (horizontal lines only, caption above)
âœ… **Interpretation:** Explain practical significance, not just p-values
âœ… **Limitations:** Acknowledge small sample size (n=5), untested hypotheses

---

## 5.7 Example Results Paragraph (Copy-Paste Template)

**"A paired-samples t-test was conducted to compare test accuracy between CrossViT-Tiny and ResNet-50 across five random seeds. There was no significant difference in accuracy between CrossViT-Tiny (M = 94.96%, SD = 0.40%) and ResNet-50 (M = 95.45%, SD = 0.39%), t(4) = -1.266, p = .277, two-tailed, d = -1.134. The 95% confidence interval for the mean difference was [-1.57%, 0.59%]. Although the effect size was large (|d| > 0.8), the result did not reach significance after Bonferroni correction (alpha' = 0.01). These findings suggest that CrossViT's architectural innovations provide no measurable accuracy advantage over traditional CNNs on this dataset."**

---

**Modify the template above with your actual results and paste into your thesis Chapter 5.**

**All numbers, p-values, and effect sizes are available in:**
- `experiments/phase3_analysis/statistical_validation/`
- `experiments/phase3_analysis/error_analysis/`
- `experiments/phase3_analysis/ablation_studies/`

**Good luck with thesis writing!** ðŸŽ“
